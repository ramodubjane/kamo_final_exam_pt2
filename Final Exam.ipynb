{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7674858,"sourceType":"datasetVersion","datasetId":4476885},{"sourceId":7675259,"sourceType":"datasetVersion","datasetId":4477162}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, TreebankWordTokenizer\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport scipy.stats as stats\n\n# Set the matplotlib inline for displaying plots in Jupyter Notebook\n%matplotlib inline\n","metadata":{"ExecuteTime":{"end_time":"2021-06-23T10:30:53.800892Z","start_time":"2021-06-23T10:30:50.215449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/exam-still/claims_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:41.381174Z","iopub.execute_input":"2024-02-22T05:27:41.381862Z","iopub.status.idle":"2024-02-22T05:27:41.408234Z","shell.execute_reply.started":"2024-02-22T05:27:41.381829Z","shell.execute_reply":"2024-02-22T05:27:41.407151Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"rand_dollar = pd.read_csv(\"/kaggle/input/exam-still/rand-dollar.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:41.409818Z","iopub.execute_input":"2024-02-22T05:27:41.410167Z","iopub.status.idle":"2024-02-22T05:27:41.420020Z","shell.execute_reply.started":"2024-02-22T05:27:41.410138Z","shell.execute_reply":"2024-02-22T05:27:41.418819Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"data = pd.read_excel(\"/kaggle/input/exam-still/matches.xlsx\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:41.424169Z","iopub.execute_input":"2024-02-22T05:27:41.424718Z","iopub.status.idle":"2024-02-22T05:27:41.731489Z","shell.execute_reply.started":"2024-02-22T05:27:41.424684Z","shell.execute_reply":"2024-02-22T05:27:41.730224Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"football_players = pd.read_csv(\"/kaggle/input/exam-still/football_players.csv\", encoding='latin1')","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:41.732927Z","iopub.execute_input":"2024-02-22T05:27:41.733271Z","iopub.status.idle":"2024-02-22T05:27:41.874444Z","shell.execute_reply.started":"2024-02-22T05:27:41.733243Z","shell.execute_reply":"2024-02-22T05:27:41.872897Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"# Split data into X and y\nX = rand_dollar.drop(columns=['ZAR/USD'])\ny = rand_dollar['ZAR/USD']\n\n# Print the shapes of X and y to confirm the separation\nprint(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:41.876558Z","iopub.execute_input":"2024-02-22T05:27:41.877204Z","iopub.status.idle":"2024-02-22T05:27:41.887340Z","shell.execute_reply.started":"2024-02-22T05:27:41.877044Z","shell.execute_reply":"2024-02-22T05:27:41.886162Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"X shape: (120, 14)\ny shape: (120,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a StandardScaler object\nscaler = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:41.889069Z","iopub.execute_input":"2024-02-22T05:27:41.889700Z","iopub.status.idle":"2024-02-22T05:27:41.900695Z","shell.execute_reply.started":"2024-02-22T05:27:41.889665Z","shell.execute_reply":"2024-02-22T05:27:41.898956Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:41.903530Z","iopub.execute_input":"2024-02-22T05:27:41.904346Z","iopub.status.idle":"2024-02-22T05:27:41.912928Z","shell.execute_reply.started":"2024-02-22T05:27:41.904293Z","shell.execute_reply":"2024-02-22T05:27:41.911340Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"# Fit the scaler to the training data and transform both the training and testing data\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:41.917656Z","iopub.execute_input":"2024-02-22T05:27:41.918420Z","iopub.status.idle":"2024-02-22T05:27:42.003560Z","shell.execute_reply.started":"2024-02-22T05:27:41.918357Z","shell.execute_reply":"2024-02-22T05:27:41.997117Z"},"trusted":true},"execution_count":139,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/1186118073.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the scaler to the training data and transform both the training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             return (\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \"\"\"\n\u001b[1;32m    822\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \"\"\"\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    876\u001b[0m                         )\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m                 raise ValueError(\n\u001b[1;32m    882\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m                 ) from complex_warning\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m         if (\n\u001b[1;32m   2152\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2008M01'"],"ename":"ValueError","evalue":"could not convert string to float: '2008M01'","output_type":"error"}]},{"cell_type":"code","source":"# Train the linear regression model\n# Create and train the model\nlm = LinearRegression()\nlm.fit(X_train[['Value of Exports (ZAR)']], y_train)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:42.005435Z","iopub.status.idle":"2024-02-22T05:27:42.006547Z","shell.execute_reply.started":"2024-02-22T05:27:42.006174Z","shell.execute_reply":"2024-02-22T05:27:42.006205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 1","metadata":{}},{"cell_type":"code","source":"# Print the intercept of the model\nprint(\"Intercept:\", lm.intercept_)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:42.008040Z","iopub.status.idle":"2024-02-22T05:27:42.009134Z","shell.execute_reply.started":"2024-02-22T05:27:42.008906Z","shell.execute_reply":"2024-02-22T05:27:42.008928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 3","metadata":{}},{"cell_type":"code","source":"slope = lm.coef_[0]\nprint(\"Slope:\", slope)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:42.010705Z","iopub.status.idle":"2024-02-22T05:27:42.011125Z","shell.execute_reply.started":"2024-02-22T05:27:42.010925Z","shell.execute_reply":"2024-02-22T05:27:42.010941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 5","metadata":{}},{"cell_type":"code","source":"# Define the value of exports\nexports = 100000\n\n# Reshape the exports value for prediction\nexports_reshaped = np.array(exports).reshape(-1, 1)\n\n# Predict the exchange rate\npredicted_exchange_rate = lm.predict(exports_reshaped)\nprint(\"Predicted Exchange Rate:\", predicted_exchange_rate[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:42.012235Z","iopub.status.idle":"2024-02-22T05:27:42.012643Z","shell.execute_reply.started":"2024-02-22T05:27:42.012453Z","shell.execute_reply":"2024-02-22T05:27:42.012469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 6","metadata":{}},{"cell_type":"code","source":"# Predict the target variable for the test set\ny_pred = lm.predict(X_test[['Value of Exports (ZAR)']])\n\n# Calculate the Mean Squared Error\nmse = np.mean((y_pred - y_test) ** 2)\nprint(\"Mean Squared Error:\", mse)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:42.013713Z","iopub.status.idle":"2024-02-22T05:27:42.014087Z","shell.execute_reply.started":"2024-02-22T05:27:42.013901Z","shell.execute_reply":"2024-02-22T05:27:42.013916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 7","metadata":{}},{"cell_type":"code","source":"# Calculate the residuals\nresiduals = y_test - y_pred\n\n# Calculate the total sum of squares\ntss = np.sum((y_test - np.mean(y_test)) ** 2)\n\n# Calculate the residual sum of squares\nrss = np.sum(residuals ** 2)\n\n# Calculate the R-squared value\nr_squared = 1 - (rss / tss)\nprint(\"R-squared value:\", r_squared)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T05:27:42.015307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 9","metadata":{}},{"cell_type":"code","source":"# Define the actual value for August 2017\nactual_value_aug_2017 = 103199.17\n\n# Predict the exchange rate for August 2017\npredicted_exchange_rate_aug_2017 = lm.predict([[actual_value_aug_2017]])\nprint(\"Predicted Exchange Rate for August 2017:\", predicted_exchange_rate_aug_2017[0])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 10","metadata":{}},{"cell_type":"code","source":"# Calculate the absolute error\nabsolute_error = abs(predicted_exchange_rate_aug_2017 - 13.23)\nprint(\"Absolute Error for August 2017 prediction:\", absolute_error)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 11","metadata":{}},{"cell_type":"code","source":"correlations = df.corr()['ZAR/USD'].abs().sort_values()\nweakest_variable = correlations.index[0]\nprint(\"Variable with the weakest linear relationship with ZAR/USD:\", weakest_variable)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 12","metadata":{}},{"cell_type":"code","source":"correlations = df.corr()['ZAR/USD'].abs().sort_values(ascending=False)\nstrongest_variable = correlations.index[1]\nprint(\"Variable with the strongest linear relationship with ZAR/USD:\", strongest_variable)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 13","metadata":{}},{"cell_type":"code","source":"df_lasso_ridge = pd.read_csv('rand-dollar.csv', index_col=0)\n\nX = df_lasso_ridge.drop(columns=['ZAR/USD'])\ny = df_lasso_ridge['ZAR/USD']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Ridge model\nridge = Ridge()\nridge.fit(X_train, y_train)\n\n\n# Train Lasso model\nlasso = Lasso(alpha=0.01)\nlasso.fit(X_train, y_train)\n\n# Predict the target variable for the training set using the Ridge model\nridge_y_pred_train = ridge.predict(X_train)\n\n# Calculate the Mean Squared Error for the training set using the Ridge model\nridge_mse_train = mean_squared_error(y_train, ridge_y_pred_train)\nprint(\"Training MSE of Ridge model:\", ridge_mse_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 14","metadata":{}},{"cell_type":"code","source":"# Calculate training MSE for Lasso model\nlasso_y_pred = lasso.predict(X_train)\nlasso_mse = mean_squared_error(y_train, lasso_y_pred)\nprint(\"Training MSE of Lasso model:\", lasso_mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 15","metadata":{}},{"cell_type":"code","source":"# Predict the target variable for the test set using the Ridge model\nridge_y_pred_test = ridge.predict(X_test)\n\n# Calculate the Mean Squared Error for the test set using the Ridge model\nridge_mse_test = mean_squared_error(y_test, ridge_y_pred_test)\nprint(\"Testing MSE of Ridge model:\", ridge_mse_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 16","metadata":{}},{"cell_type":"code","source":"# Predict the target variable for the test set using the Lasso model\nlasso_y_pred_test = lasso.predict(X_test)\n\n# Calculate the Mean Squared Error for the test set using the Lasso model\nlasso_mse_test = mean_squared_error(y_test, lasso_y_pred_test)\nprint(\"Testing MSE of Lasso model:\", lasso_mse_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 17","metadata":{}},{"cell_type":"code","source":"# Get the absolute values of the coefficients of the Ridge model\nridge_coefficients = abs(ridge.coef_)\n\n# Find the index of the indicator with the highest coefficient\nbest_predictor_index = ridge_coefficients.argmax()\n\n# Get the name of the best predictor\nbest_predictor = X.columns[best_predictor_index]\n\nprint(\"Best predictor based on Ridge model's coefficients:\", best_predictor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 18","metadata":{}},{"cell_type":"code","source":"# Get the absolute values of the coefficients of the Ridge model\nridge_coefficients = abs(ridge.coef_)\n\n# Find the index of the indicator with the lowest coefficient\nworst_predictor_index = ridge_coefficients.argmin()\n\n# Get the name of the worst predictor\nworst_predictor = X.columns[worst_predictor_index]\n\nprint(\"Worst predictor based on Ridge model's coefficients:\", worst_predictor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 19","metadata":{}},{"cell_type":"code","source":"# Get the absolute values of the coefficients of the Lasso model\nlasso_coefficients = abs(lasso.coef_)\n\n# Find the index of the indicator with the highest coefficient\nbest_predictor_index = lasso_coefficients.argmax()\n\n# Get the name of the best predictor\nbest_predictor = X.columns[best_predictor_index]\n\nprint(\"Best predictor based on Lasso model's coefficients:\", best_predictor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 20","metadata":{}},{"cell_type":"code","source":"# Count the number of coefficients equal to zero in the Lasso model\nzero_coefficients_count = sum(lasso.coef_ == 0)\n\nprint(\"Number of variables with coefficients equal to zero in the Lasso model:\", zero_coefficients_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 21","metadata":{}},{"cell_type":"code","source":"#Filter individuals with BMI greater than 25\noverweight_or_obese = df[df['bmi'] > 25]\n\n# Calculate proportion of individuals with BMI greater than 25\nproportion_overweight_or_obese = len(overweight_or_obese) / len(df)\n\nprint(\"Proportion of individuals classified as overweight or obese (BMI > 25):\", proportion_overweight_or_obese)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 22","metadata":{}},{"cell_type":"code","source":"# Calculate mean and variance of the 'children' column\nmean_children = df['children'].mean()\nvar_children = df['children'].var()\n\nprint(\"Mean of 'children' column:\", mean_children)\nprint(\"Variance of 'children' column:\", var_children)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 23","metadata":{}},{"cell_type":"code","source":"# Calculate mean and standard deviation of the 'age' column from the dataset\nmean_age = df['age'].mean()\nstd_age = df['age'].std()\n\n# Calculate the probability of being aged 60 or older using the cumulative distribution function (CDF)\nprob_60_or_older = 1 - stats.norm.cdf(60, loc=mean_age, scale=std_age)\n\n# Number of individuals in the dataset\ntotal_individuals = len(df['age'])\n\n# Expected number of individuals aged 60 or older\nexpected_60_or_older = round(total_individuals * prob_60_or_older)\n\n# Count of individuals aged 60 or older in the dataset\nactual_60_or_older = sum(1 for age in df['age'] if age >= 60)\n\nprint(\"Expected number of individuals aged 60 or older:\", expected_60_or_older)\nprint(\"Actual number of individuals aged 60 or older:\", actual_60_or_older)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 24","metadata":{}},{"cell_type":"code","source":"# Create joint plot\njoint = sns.jointplot(x='age', y='bmi', data=df, kind='scatter')\n\n# Calculate correlation coefficient\ncorrelation_coefficient = df['age'].corr(df['bmi'])\n\n# Annotate the plot with correlation coefficient\njoint.ax_joint.annotate(f'Corr: {correlation_coefficient:.2f}', xy=(40, 50), fontsize=12)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 25","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Assuming df contains the dataset with features and target variable\n\n# Convert target variable to binary (1 indicates a claim)\ndf['insurance_claim'] = df['insurance_claim'].apply(lambda x: 1 if x == 'yes' else 0)\n\n# Separate features and target variable\nX = df.drop(['insurance_claim', 'claim_amount'], axis=1)\ny = df['insurance_claim']\n\n# Define categorical features\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\n\n# Define preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ],\n    remainder='passthrough'\n)\n\n# Define the logistic regression model\nmodel = LogisticRegression()\n\n# Create a pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', model)\n])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Fit the pipeline to the training data\npipeline.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = pipeline.predict(X_test)\n\n# Calculate the proportion of correctly predicted claim indicators\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Proportion of correctly predicted claim indicators:\", accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 26","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Convert target variable to binary (1 indicates a claim)\ndf['insurance_claim'] = df['insurance_claim'].apply(lambda x: 1 if x == 'yes' else 0)\n\n# Separate features and target variable\nX = df.drop(['insurance_claim', 'claim_amount'], axis=1)\ny = df['insurance_claim']\n\n# Create dummy variables for categorical features\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\nct = ColumnTransformer(\n    transformers=[('cat', OneHotEncoder(drop='first'), categorical_features)],\n    remainder='passthrough'\n)\nX = ct.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Add a constant to X matrices\nX_train = sm.add_constant(X_train)\nX_test = sm.add_constant(X_test)\n\n# Fit logistic regression using statsmodels\nlogit_model = sm.Logit(y_train, X_train)\nresult = logit_model.fit()\n\n# Print summary of the logistic regression model\nprint(result.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 28","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# Convert target variable to binary (1 indicates a claim)\ndf['insurance_claim'] = df['insurance_claim'].apply(lambda x: 1 if x == 'yes' else 0)\n\n# Separate features and target variable\nX = df.drop(['insurance_claim', 'claim_amount'], axis=1)\ny = df['insurance_claim']\n\n# Create dummy variables for categorical features if needed\n# Create dummy variables for categorical features\nX = pd.get_dummies(X, drop_first=True)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Fit Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=101)\nrf_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf_classifier.predict(X_test)\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n\n\n# Extract TP, TN, FP, FN values\nTN, FP, FN, TP = conf_matrix.ravel()\n\nprint(\"False Positives (FP):\", FP)\nprint(\"False Negatives (FN):\", FN)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 29","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n\n# Convert target variable to binary (1 indicates a claim)\ndf['insurance_claim'] = df['insurance_claim'].apply(lambda x: 1 if x == 'yes' else 0)\n\n# Separate features and target variable\nX = df.drop(['insurance_claim', 'claim_amount'], axis=1)\ny = df['insurance_claim']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Fit SVM models with different kernels\nsvm_radial = SVC(kernel='rbf')  # Radial kernel\nsvm_sigmoid = SVC(kernel='sigmoid')  # Sigmoid kernel\nsvm_linear = SVC(kernel='linear')  # Linear kernel\n\n# Fit SVM models to the training data\nsvm_radial.fit(X_train, y_train)\nsvm_sigmoid.fit(X_train, y_train)\nsvm_linear.fit(X_train, y_train)\n\n# Predictions\ny_pred_radial = svm_radial.predict(X_test)\ny_pred_sigmoid = svm_sigmoid.predict(X_test)\ny_pred_linear = svm_linear.predict(X_test)\n\n# Calculate accuracy scores\naccuracy_radial = accuracy_score(y_test, y_pred_radial)\naccuracy_sigmoid = accuracy_score(y_test, y_pred_sigmoid)\naccuracy_linear = accuracy_score(y_test, y_pred_linear)\n\nprint(\"Accuracy score for radial kernel:\", accuracy_radial)\nprint(\"Accuracy score for sigmoid kernel:\", accuracy_sigmoid)\nprint(\"Accuracy score for linear kernel:\", accuracy_linear)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 30","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the file path\nfile_path = 'matches.xlsx'\n\n# Read the Excel file into a DataFrame\ndata = pd.read_excel(file_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the proportion of matches with Duckworth-Lewis applied\ndl_applied_proportion = data['dl_applied'].mean()\n\nprint(f\"The proportion of matches with Duckworth-Lewis applied: {dl_applied_proportion:.2%}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filter matches where the team batting first is the winner\nmatches_won_by_batting_first = data[data['team1'] == data['winner']]\n\n# Calculate the proportion of matches won by the team batting first\nproportion_won_by_batting_first = len(matches_won_by_batting_first) / len(data)\n\nprint(f\"The proportion of matches won by the team batting first: {proportion_won_by_batting_first:.2%}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the 'date' column to datetime format\ndata['date'] = pd.to_datetime(data['date'])\n# 1. Create the feature indicating whether the match was played in April or not\ndata['is_april'] = data['date'].dt.month == 4\n\n\n\n# 3. Count the number of April games and choices to field first\napril_games_count = data['is_april'].sum()\n\n\nprint(\"Number of April games:\", april_games_count)\n\n# 1. Create the feature indicating whether the toss decision was to field or not\ndata['toss_decision_field'] = data['toss_decision'] == 'field'\n\n# Count the number of times the toss decision was to field\nfield_decision_count = data['toss_decision_field'].sum()\n\nprint(\"Number of times the toss decision was to field:\", field_decision_count)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n\n\n# Convert the 'date' column to datetime format\ndata['date'] = pd.to_datetime(data['date'])\n\n# 1. Create the feature indicating whether the match was played in April or not\ndata['is_april'] = data['date'].dt.month == 4\n\n# 1. Create the feature indicating whether the toss decision was to field or not\ndata['toss_decision_field'] = data['toss_decision'] == 'field'\n\n# Select the features and target variable\nfeatures = ['is_april', 'toss_decision_field']\ntarget_variable = 'toss_winner'  # Replace with the actual target variable name\nX = data[features]\ny = data[target_variable]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=999)\n\n# Build the Decision Tree classifier\ndecision_tree_classifier = DecisionTreeClassifier(random_state=999)\ndecision_tree_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = decision_tree_classifier.predict(X_test)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy of the Decision Tree classifier:\", accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 35","metadata":{}},{"cell_type":"code","source":"encodings = ['utf-8', 'latin1', 'ISO-8859-1']\n\nfor encoding in encodings:\n    try:\n        Fifa = pd.read_csv('football_players.csv', encoding=encoding)\n        print(\"File read successfully with encoding:\", encoding)\n        break  # Stop trying encodings once successful\n    except UnicodeDecodeError:\n        print(\"Failed to read file with encoding:\", encoding)\n\n# Further processing of the Fifa DataFrame as needed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Fifa.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_common_overall = Fifa['Overall'].value_counts().idxmax()\n\nprint(\"The most common Overall score for players in the database is:\", most_common_overall)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 36","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# 1. Subset the dataset for central defenders\ncentral_defenders = Fifa[Fifa['Preferred Positions'].str.contains('CB')]\n\n# 2. Create a new target variable based on Overall score\ndef classify_overall(overall_score):\n    if overall_score >= 80:\n        return 'World Class'\n    elif 70 <= overall_score < 80:\n        return 'Good'\n    else:\n        return 'Mediocre'\n\ncentral_defenders['Overall_Class'] = central_defenders['Overall'].apply(classify_overall)\n\n# 3. Prepare data for modeling\nX = central_defenders.select_dtypes(include=[np.number]).drop(['Overall'], axis=1)\ny = central_defenders['Overall_Class']\n\n# 4. Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1971)\n\n# 5. Build the Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=500, random_state=1971)\nrf_classifier.fit(X_train, y_train)\n\n# 6. Get feature importances\nfeature_importances = pd.Series(rf_classifier.feature_importances_, index=X.columns)\n\n# 7. Display the top 5 important features\ntop_5_features = feature_importances.sort_values(ascending=False).head(5)\nprint(\"Top 5 important features:\")\nprint(top_5_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 38","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score\n\n# Assuming you have already prepared your dataset and defined your features (X) and target variable (y)\n\n# 1. Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=911)\n\n# 2. Define a list of k values to try\nk_values = [1, 2, 3, 4, 5]\n\n# 3. Initialize dictionaries to store F1 scores for each k value\nf1_scores_world_class = {}\nf1_scores_good = {}\n\n# 4. Train KNN models with different k values and evaluate F1 scores\nfor k in k_values:\n    # Train KNN model\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    \n    # Predictions\n    y_pred = knn.predict(X_test)\n    \n    # Calculate F1 scores for \"World Class\" and \"Good\" groups respectively\n    f1_score_world_class = f1_score(y_test[y_test == 'World Class'], y_pred[y_test == 'World Class'], average='weighted')\n    f1_score_good = f1_score(y_test[y_test == 'Good'], y_pred[y_test == 'Good'], average='weighted')\n    \n    # Store F1 scores in dictionaries\n    f1_scores_world_class[k] = f1_score_world_class\n    f1_scores_good[k] = f1_score_good\n\n# 5. Determine the best k value for each group based on the highest F1 score\nbest_k_world_class = max(f1_scores_world_class, key=f1_scores_world_class.get)\nbest_k_good = max(f1_scores_good, key=f1_scores_good.get)\n\nprint(\"Best k value for World Class group:\", best_k_world_class)\nprint(\"Best k value for Good group:\", best_k_good)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 41","metadata":{}},{"cell_type":"code","source":"essay = pd.read_csv(\"/kaggle/input/essay-data/Essay_data.csv\")","metadata":{"ExecuteTime":{"end_time":"2021-06-28T08:49:35.311495Z","start_time":"2021-06-28T08:49:35.295494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"essay.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the shape of the DataFrame before dropping missing values\nprint(\"Shape of DataFrame before dropping missing values:\", essay.shape)\n\n# Remove rows with missing values and reset index\nessay = essay.dropna().reset_index(drop=True)\n\n# Print the shape of the DataFrame after dropping missing values\nprint(\"Shape of DataFrame after dropping missing values:\", essay.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 42","metadata":{}},{"cell_type":"code","source":"from nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport string\n\n# Original sentence\nsentence = \"I’m a part-time student @explore-software.\"\n\n# Step 1: Remove stopwords\nstop_words = set(stopwords.words('english'))\nfiltered_sentence = [word for word in sentence.split() if word.lower() not in stop_words]\n\n# Step 2: Remove punctuation and replace by a single white space\npunctuations = string.punctuation\ncleaned_sentence = ''.join([char if char not in punctuations else ' ' for char in sentence])\n\n# Step 3: Convert all text to lower case\nlower_case_sentence = cleaned_sentence.lower()\n\n# Tokenize the sentence into words using NLTK's word_tokenize function\nwords = word_tokenize(lower_case_sentence)\n\n# Remove single-character words and \"i'm\"\nwords = [word for word in words if len(word) > 1 and word != \"i’m\"]\n\n# Count bi-grams\nbi_grams = list(ngrams(words, 2))\n\n# Corrected count of bi-grams\ncorrect_bi_gram_count = len(bi_grams)\n\n# Print bi-grams and correct count\nprint(\"Bi-grams:\", bi_grams)\nprint(\"Correct number of bi-grams:\", correct_bi_gram_count)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 43","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom fractions import Fraction\n\n# Load the essay data\nessay = pd.read_csv(\"/kaggle/input/essay-data/Essay_data.csv\")\n\n# Count the occurrences of 'N' and 'S' in the 'N/S' column\nn_count = essay['N/S'].value_counts().get('N', 0)\ns_count = essay['N/S'].value_counts().get('S', 0)\n\n# Calculate the ratio of N students to S students\nif s_count != 0:\n    ratio_n_to_s = Fraction(n_count, s_count)\n    print(\"The ratio of intuitive (N) students to sensing (S) students is:\", f\"{ratio_n_to_s.numerator}:{ratio_n_to_s.denominator}\")\nelse:\n    print(\"The ratio of intuitive (N) students to sensing (S) students is undefined because there are no sensing (S) students.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 44","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Load the essay data\nessay_data = pd.read_csv(\"/kaggle/input/essay-data/Essay_data.csv\")\n\n# Function to remove punctuation and convert to lower case\ndef remove_punctuation_and_lower(text):\n    # Remove punctuation using regular expression\n    text_no_punctuation = re.sub(r'[^\\w\\s]', '', text)\n    # Convert to lower case\n    text_lower = text_no_punctuation.lower()\n    return text_lower\n\n# Apply the function to each essay in the 'Essay' column\nessay_data['Essay'] = essay_data['Essay'].apply(remove_punctuation_and_lower)\n\n# Display the updated DataFrame\nprint(essay_data.head())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the text of the first essay\nfirst_essay_text = essay_data['Essay'][0]\n\n# Get the 10th character (index 9) in the first essay\ntenth_character = first_essay_text[9]\n\nprint(\"The 10th character in the first essay is:\", tenth_character)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 45","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Load the essay data\nessay_data = pd.read_csv(\"/kaggle/input/essay-data/Essay_data.csv\")\n\n# Tokenize function\ndef tokenize_essay(text):\n    # Tokenize the text\n    tokens = word_tokenize(text)\n    return tokens\n\n# Tokenize each essay in the 'Essay' column\nessay_data['Tokenized_Essay'] = essay_data['Essay'].apply(tokenize_essay)\n\n# Display the DataFrame with tokenized essays\nprint(essay_data[['Essay', 'Tokenized_Essay']].head())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the list of tokens for the 17th essay\ntokens_17th_essay = essay_data['Tokenized_Essay'][16]\n\n# Get the number of tokens in the list\nnum_tokens_17th_essay = len(tokens_17th_essay)\n\nprint(\"The number of tokens in the 17th essay is:\", num_tokens_17th_essay)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 47","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import SnowballStemmer\n\n# Initialize SnowballStemmer for English\nstemmer = SnowballStemmer('english')\n\n# Stem the word \"experiences\"\nstemmed_word = stemmer.stem('experiences')\n\nprint(\"Stemmed word:\", stemmed_word)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 48","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Load the essay data\nessay_data = pd.read_csv(\"/kaggle/input/essay-data/Essay_data.csv\")\n\n# Tokenize function\ndef tokenize_essay(text):\n    # Tokenize the text\n    tokens = word_tokenize(text)\n    return tokens\n\n# Remove stop words function\ndef remove_stop_words(tokens):\n    # Get English stop words\n    stop_words = set(stopwords.words('english'))\n    # Remove stop words\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n    return filtered_tokens\n\n# Tokenize and remove stop words for each essay\nessay_data['Tokenized_Essay'] = essay_data['Essay'].apply(tokenize_essay)\nessay_data['Filtered_Tokens'] = essay_data['Tokenized_Essay'].apply(remove_stop_words)\n\n# Get the 24th token in the 81st essay\ntoken_81st_essay = essay_data['Filtered_Tokens'][80][23]\n\nprint(\"The 24th token in the 81st essay is:\", token_81st_essay)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 49","metadata":{}},{"cell_type":"code","source":"# Concatenate all filtered tokens from the essays\nall_filtered_tokens = [token for tokens_list in essay_data['Filtered_Tokens'] for token in tokens_list]\n\n# Get the number of unique words\nnum_unique_words = len(set(all_filtered_tokens))\n\nprint(\"The number of unique words in the essays after removing stopwords is:\", num_unique_words)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 50","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n# Tokenize the text of the 56th essay\ntokens_56th_essay = essay_data['Filtered_Tokens'][55]\n\n# Create a bag of words (word counts)\nbag_of_words_56th_essay = Counter(tokens_56th_essay)\n\n# Get the count of 'time' in the 56th essay\ncount_time_56th_essay = bag_of_words_56th_essay.get('time', 0)  # Get the count of 'time', default to 0 if not found\n\nprint(\"The number of times 'time' was mentioned in the 56th essay is:\", count_time_56th_essay)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 51","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n# Concatenate all filtered tokens from the essays\nall_filtered_tokens = [token for tokens_list in essay_data['Filtered_Tokens'] for token in tokens_list]\n\n# Count the occurrences of each word\nword_counts = Counter(all_filtered_tokens)\n\n# Count the number of unique words that appear at least twice\nnum_unique_words_at_least_twice = sum(1 for count in word_counts.values() if count >= 2)\n\n# Count the total number of words in the essays\ntotal_words = len(all_filtered_tokens)\n\n# Calculate the percentage of unique words that appear at least twice\npercentage_words_at_least_twice = (num_unique_words_at_least_twice / len(word_counts)) * 100\n\nprint(\"Percentage of words that appear at least twice in the essays:\", percentage_words_at_least_twice)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 52","metadata":{}},{"cell_type":"code","source":"import string\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Filter essays by ENFJ personalities (J/P = J)\nenfj_essays = essay_data.loc[essay_data['J/P'] == 'J', 'Essay']\n\n# Concatenate all essays\nenfj_text = ' '.join(enfj_essays)\n\n# Tokenize the text and remove stopwords and punctuation\nenfj_tokens = [word.lower() for word in word_tokenize(enfj_text) if word.lower() not in stopwords.words('english') and word.isalnum()]\n\n# Count the occurrences of each word\nword_counts = Counter(enfj_tokens)\n\n# Find the most commonly mentioned word\nmost_common_word = word_counts.most_common(1)[0][0]\n\nprint(\"The most commonly mentioned word by ENFJ personalities after removing stop words and punctuation is:\", most_common_word)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 53","metadata":{}},{"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Define a function to generate bi-grams from text after removing stopwords and punctuation\ndef generate_bigrams(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())  # Convert text to lowercase\n    \n    # Remove stop words and punctuation\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n    \n    # Generate bi-grams\n    bigrams = [(filtered_tokens[i], filtered_tokens[i+1]) for i in range(len(filtered_tokens)-1)]\n    return bigrams\n\n# Apply the function to the 70th essay and store the bi-grams\nbi_grams_70th_essay = generate_bigrams(essay_data.loc[70, 'Essay'])\n\n# Get the 109th bi-gram in the 70th essay\nbi_gram_109_70_essay = bi_grams_70th_essay[109]\n\nprint(\"The 109th bi-gram in the 70th essay after removing stopwords and punctuation is:\", bi_gram_109_70_essay)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}